{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbcbd04",
   "metadata": {},
   "source": [
    "## Twitter sentiment analysis with BART Comparaison - Machine Learning models notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9becd4e0",
   "metadata": {},
   "source": [
    "### Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494c2c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/photoli93/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/photoli93/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4d01b",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129a3f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_positive</th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>user</th>\n",
       "      <th>message</th>\n",
       "      <th>bart_is_positive</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>capital_letters_count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>bart_pred</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5307</th>\n",
       "      <td>1</td>\n",
       "      <td>1827937592</td>\n",
       "      <td>Sun May 17 11:45:24 PDT 2009</td>\n",
       "      <td>PansyMariee</td>\n",
       "      <td>For my 15th Birthday the one thing i want to d...</td>\n",
       "      <td>0.423072</td>\n",
       "      <td>127</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>3.535714</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15th birthday one thing want give money earned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>1</td>\n",
       "      <td>2177052206</td>\n",
       "      <td>Mon Jun 15 05:20:50 PDT 2009</td>\n",
       "      <td>annisatadiyana</td>\n",
       "      <td>#musicmonday a lot of songs for today</td>\n",
       "      <td>0.228707</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>musicmonday lot songs today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17479</th>\n",
       "      <td>1</td>\n",
       "      <td>2011971974</td>\n",
       "      <td>Tue Jun 02 19:51:46 PDT 2009</td>\n",
       "      <td>Vicki_McGuire</td>\n",
       "      <td>NYC here we come</td>\n",
       "      <td>0.746840</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>nyc come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1</td>\n",
       "      <td>1992708729</td>\n",
       "      <td>Mon Jun 01 09:11:41 PDT 2009</td>\n",
       "      <td>zsoczi02</td>\n",
       "      <td>MTV Movie Awards yesterday: congrats Robert, K...</td>\n",
       "      <td>0.839815</td>\n",
       "      <td>93</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mtv movie awards yesterday congrats robert kri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>1</td>\n",
       "      <td>2063001310</td>\n",
       "      <td>Sun Jun 07 01:01:26 PDT 2009</td>\n",
       "      <td>BrandyWandLover</td>\n",
       "      <td>@ScruffyPanther cool  i was never good at lang...</td>\n",
       "      <td>0.602748</td>\n",
       "      <td>81</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4.062500</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>scruffypanther cool never good languages got c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_positive          id                      datetime             user  \\\n",
       "5307             1  1827937592  Sun May 17 11:45:24 PDT 2009      PansyMariee   \n",
       "2221             1  2177052206  Mon Jun 15 05:20:50 PDT 2009   annisatadiyana   \n",
       "17479            1  2011971974  Tue Jun 02 19:51:46 PDT 2009    Vicki_McGuire   \n",
       "417              1  1992708729  Mon Jun 01 09:11:41 PDT 2009         zsoczi02   \n",
       "5428             1  2063001310  Sun Jun 07 01:01:26 PDT 2009  BrandyWandLover   \n",
       "\n",
       "                                                 message  bart_is_positive  \\\n",
       "5307   For my 15th Birthday the one thing i want to d...          0.423072   \n",
       "2221              #musicmonday a lot of songs for today           0.228707   \n",
       "17479                                  NYC here we come           0.746840   \n",
       "417    MTV Movie Awards yesterday: congrats Robert, K...          0.839815   \n",
       "5428   @ScruffyPanther cool  i was never good at lang...          0.602748   \n",
       "\n",
       "       text_length  word_count  sentence_count  avg_word_length  \\\n",
       "5307           127          28               1         3.535714   \n",
       "2221            38           7               1         4.428571   \n",
       "17479           17           4               1         3.250000   \n",
       "417             93          13               2         6.153846   \n",
       "5428            81          16               2         4.062500   \n",
       "\n",
       "       punctuation_count  capital_letters_count  exclamation_count  \\\n",
       "5307                   0                      2                  0   \n",
       "2221                   1                      0                  0   \n",
       "17479                  0                      3                  0   \n",
       "417                    7                      9                  0   \n",
       "5428                   3                      7                  1   \n",
       "\n",
       "       question_count  url_count  mention_count  hashtag_count  bart_pred  \\\n",
       "5307                0          0              0              0          0   \n",
       "2221                0          0              0              1          0   \n",
       "17479               0          0              0              0          1   \n",
       "417                 0          0              0              0          1   \n",
       "5428                0          0              1              0          1   \n",
       "\n",
       "                                              clean_text  \n",
       "5307   15th birthday one thing want give money earned...  \n",
       "2221                         musicmonday lot songs today  \n",
       "17479                                           nyc come  \n",
       "417    mtv movie awards yesterday congrats robert kri...  \n",
       "5428   scruffypanther cool never good languages got c...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   is_positive            20000 non-null  int64  \n",
      " 1   id                     20000 non-null  int64  \n",
      " 2   datetime               20000 non-null  object \n",
      " 3   user                   20000 non-null  object \n",
      " 4   message                20000 non-null  object \n",
      " 5   bart_is_positive       20000 non-null  float64\n",
      " 6   text_length            20000 non-null  int64  \n",
      " 7   word_count             20000 non-null  int64  \n",
      " 8   sentence_count         20000 non-null  int64  \n",
      " 9   avg_word_length        20000 non-null  float64\n",
      " 10  punctuation_count      20000 non-null  int64  \n",
      " 11  capital_letters_count  20000 non-null  int64  \n",
      " 12  exclamation_count      20000 non-null  int64  \n",
      " 13  question_count         20000 non-null  int64  \n",
      " 14  url_count              20000 non-null  int64  \n",
      " 15  mention_count          20000 non-null  int64  \n",
      " 16  hashtag_count          20000 non-null  int64  \n",
      " 17  bart_pred              20000 non-null  int64  \n",
      " 18  clean_text             20000 non-null  object \n",
      "dtypes: float64(2), int64(13), object(4)\n",
      "memory usage: 2.9+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_positive</th>\n",
       "      <th>id</th>\n",
       "      <th>bart_is_positive</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>capital_letters_count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>bart_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.502900</td>\n",
       "      <td>1.999831e+09</td>\n",
       "      <td>0.308301</td>\n",
       "      <td>74.321800</td>\n",
       "      <td>13.209500</td>\n",
       "      <td>2.354150</td>\n",
       "      <td>4.868268</td>\n",
       "      <td>4.006500</td>\n",
       "      <td>3.266950</td>\n",
       "      <td>0.573750</td>\n",
       "      <td>0.15770</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.493950</td>\n",
       "      <td>0.026250</td>\n",
       "      <td>0.30550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500004</td>\n",
       "      <td>1.938426e+08</td>\n",
       "      <td>0.353417</td>\n",
       "      <td>36.389079</td>\n",
       "      <td>6.952138</td>\n",
       "      <td>1.232357</td>\n",
       "      <td>1.350713</td>\n",
       "      <td>3.487702</td>\n",
       "      <td>5.251816</td>\n",
       "      <td>1.405119</td>\n",
       "      <td>1.08724</td>\n",
       "      <td>0.210374</td>\n",
       "      <td>0.595215</td>\n",
       "      <td>0.182654</td>\n",
       "      <td>0.46063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467816e+09</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.957074e+09</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.002057e+09</td>\n",
       "      <td>0.110693</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.177694e+09</td>\n",
       "      <td>0.638163</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.285714</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.329179e+09</td>\n",
       "      <td>0.998371</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>45.333333</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>88.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_positive            id  bart_is_positive   text_length  \\\n",
       "count  20000.000000  2.000000e+04      20000.000000  20000.000000   \n",
       "mean       0.502900  1.999831e+09          0.308301     74.321800   \n",
       "std        0.500004  1.938426e+08          0.353417     36.389079   \n",
       "min        0.000000  1.467816e+09          0.000089      7.000000   \n",
       "25%        0.000000  1.957074e+09          0.004287     44.000000   \n",
       "50%        1.000000  2.002057e+09          0.110693     70.000000   \n",
       "75%        1.000000  2.177694e+09          0.638163    104.000000   \n",
       "max        1.000000  2.329179e+09          0.998371    222.000000   \n",
       "\n",
       "         word_count  sentence_count  avg_word_length  punctuation_count  \\\n",
       "count  20000.000000    20000.000000     20000.000000       20000.000000   \n",
       "mean      13.209500        2.354150         4.868268           4.006500   \n",
       "std        6.952138        1.232357         1.350713           3.487702   \n",
       "min        1.000000        1.000000         1.428571           0.000000   \n",
       "25%        7.000000        1.000000         4.090909           2.000000   \n",
       "50%       12.000000        2.000000         4.600000           3.000000   \n",
       "75%       19.000000        3.000000         5.285714           6.000000   \n",
       "max       41.000000       21.000000        45.333333          89.000000   \n",
       "\n",
       "       capital_letters_count  exclamation_count  question_count     url_count  \\\n",
       "count           20000.000000       20000.000000     20000.00000  20000.000000   \n",
       "mean                3.266950           0.573750         0.15770      0.044100   \n",
       "std                 5.251816           1.405119         1.08724      0.210374   \n",
       "min                 0.000000           0.000000         0.00000      0.000000   \n",
       "25%                 1.000000           0.000000         0.00000      0.000000   \n",
       "50%                 2.000000           0.000000         0.00000      0.000000   \n",
       "75%                 4.000000           1.000000         0.00000      0.000000   \n",
       "max               105.000000          74.000000        88.00000      4.000000   \n",
       "\n",
       "       mention_count  hashtag_count    bart_pred  \n",
       "count   20000.000000   20000.000000  20000.00000  \n",
       "mean        0.493950       0.026250      0.30550  \n",
       "std         0.595215       0.182654      0.46063  \n",
       "min         0.000000       0.000000      0.00000  \n",
       "25%         0.000000       0.000000      0.00000  \n",
       "50%         0.000000       0.000000      0.00000  \n",
       "75%         1.000000       0.000000      1.00000  \n",
       "max         9.000000       5.000000      1.00000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = \"../data/dataset_twitter_classification.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "display(df.sample(5))\n",
    "print(df.info())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d6e5c",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef6cf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing Examples ===\n",
      "Original: @AbhorrentAspen I am  Guess I gotta find someone else then.\n",
      "Processed: abhorrentaspen guess got find someone else\n",
      "--------------------------------------------------\n",
      "Original: studing for my last exam for the semester \n",
      "Processed: studing last exam semester\n",
      "--------------------------------------------------\n",
      "Original: haha i had so much fun last night! eating whip cream at 2 in the morning and watching Invader Zim and having a party by myself haha \n",
      "Processed: haha much fun last night eating whip cream 2 morning watching invader zim party haha\n",
      "--------------------------------------------------\n",
      "Original: I recommend returning your broken duck toy with the batteis it came with or the person who your giving it to may get mad!!  trust me!\n",
      "Processed: recommend returning broken duck toy batteis came person giving may get mad trust\n",
      "--------------------------------------------------\n",
      "Original: @thekelliejane sadly, this is a grocery store, not a restaurant.  might it still apply?\n",
      "Processed: thekelliejane sadly grocery store restaurant might still apply\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = SpellChecker()\n",
    "\n",
    "def remove_html_artefacts(text):\n",
    "    \"\"\"Remove HTML artefacts amp, lt, gt\"\"\"\n",
    "    # On peut ajouter d'autres artefacts si besoin\n",
    "    artifacts = [\"amp\", \"lt\", \"gt\"]\n",
    "    pattern = r'\\b(' + '|'.join(artifacts) + r')\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    \"\"\"Correct spelling for each token\"\"\"\n",
    "    corrected_tokens = []\n",
    "    for token in tokens:\n",
    "        # Skip Twitter mentions and <URL>\n",
    "        if token.startswith('@') or token == '':\n",
    "            corrected_tokens.append(token)\n",
    "        else:\n",
    "            corrected_tokens.append(spell.correction(token))\n",
    "    return corrected_tokens\n",
    "\n",
    "def tokenize_and_process(text, use_stemming=True, remove_stopwords=True):\n",
    "    \"\"\"Tokenize text and apply stemming or lemmatization\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove HTML artefacts\n",
    "    text = remove_html_artefacts(text)\n",
    "\n",
    "    # Temporarily protect <URL> and <EMAIL>\n",
    "    text = text.replace(\"<URL>\", \"URLTOKEN\").replace(\"<EMAIL>\", \"EMAILTOKEN\")\n",
    "\n",
    "    # Tokenize texts\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Restore <URL> and <EMAIL> in tokens\n",
    "    tokens = [\"<URL>\" if t == \"URLTOKEN\" else (\"<EMAIL>\" if t == \"EMAILTOKEN\" else t) for t in tokens]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    if use_stemming:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    else:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess(texts, use_stemming=True, remove_stopwords=True):\n",
    "    \"\"\"Apply full preprocessing pipeline to a list of texts\"\"\"\n",
    "    return [\n",
    "        tokenize_and_process(text, use_stemming, remove_stopwords)\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "df['processed_text'] = preprocess(df['clean_text'].tolist(), use_stemming=False)\n",
    "\n",
    "print(\"\\n=== Preprocessing Examples ===\")\n",
    "for i, row in df.sample(5).iterrows():\n",
    "    print(f\"Original: {row['message']}\")\n",
    "    print(f\"Processed: {row['processed_text']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15babbc9",
   "metadata": {},
   "source": [
    "### Feature Engineering and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d76daa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 15999\n",
      "Test set size: 4000\n",
      "Training set positive ratio: 0.503\n",
      "Test set positive ratio: 0.503\n"
     ]
    }
   ],
   "source": [
    "# Train_test_split\n",
    "def prepare_features(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Prepare features and split data\"\"\"\n",
    "    # Remove empty processed texts\n",
    "    df_clean = df[df['processed_text'].str.len() > 0].copy()\n",
    "    \n",
    "    X = df_clean['processed_text']\n",
    "    y = df_clean['is_positive']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    print(f\"Training set positive ratio: {y_train.mean():.3f}\")\n",
    "    print(f\"Test set positive ratio: {y_test.mean():.3f}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, df_clean\n",
    "\n",
    "X_train, X_test, y_train, y_test, df_clean = prepare_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa5db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_df=0.8, max_features=10000, min_df=2,\n",
      "                                 ngram_range=(1, 2), stop_words='english')),\n",
      "                ('clf', MultinomialNB(alpha=0.1))])\n",
      "Naive Bayes: mean F1 = 0.7059 ± 0.0072 (time: 1.00 sec)\n",
      "Logistic Regression\n",
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_df=0.8, max_features=10000, min_df=2,\n",
      "                                 ngram_range=(1, 2), stop_words='english')),\n",
      "                ('clf', LogisticRegression(max_iter=1000, random_state=42))])\n",
      "Logistic Regression: mean F1 = 0.7313 ± 0.0038 (time: 1.07 sec)\n",
      "Random Forest\n",
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_df=0.8, max_features=10000, min_df=2,\n",
      "                                 ngram_range=(1, 2), stop_words='english')),\n",
      "                ('clf', RandomForestClassifier(random_state=42))])\n",
      "Random Forest: mean F1 = 0.7124 ± 0.0053 (time: 33.93 sec)\n",
      "\n",
      "Training Naive Bayes on full training set\n",
      "Naive Bayes finished in 0.27 sec\n",
      "\n",
      "Training Logistic Regression on full training set\n",
      "Logistic Regression finished in 0.31 sec\n",
      "\n",
      "Training Random Forest on full training set\n",
      "Random Forest finished in 9.01 sec\n",
      "\n",
      "=== Final Model Performance (with timings) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>CV F1 (μ±σ)</th>\n",
       "      <th>CV Time (s)</th>\n",
       "      <th>Train Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.7092</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7059±0.0072</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.7292</td>\n",
       "      <td>0.7265</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.7334</td>\n",
       "      <td>0.8047</td>\n",
       "      <td>0.7313±0.0038</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.7057</td>\n",
       "      <td>0.7038</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.7102</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>0.7124±0.0053</td>\n",
       "      <td>33.93</td>\n",
       "      <td>9.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Accuracy Precision  Recall F1-Score ROC-AUC  \\\n",
       "Naive Bayes           0.7127    0.7167  0.7092   0.7130  0.7755   \n",
       "Logistic Regression   0.7292    0.7265  0.7406   0.7334  0.8047   \n",
       "Random Forest         0.7057    0.7038  0.7167   0.7102  0.7827   \n",
       "\n",
       "                       CV F1 (μ±σ) CV Time (s) Train Time (s)  \n",
       "Naive Bayes          0.7059±0.0072        1.00           0.27  \n",
       "Logistic Regression  0.7313±0.0038        1.07           0.31  \n",
       "Random Forest        0.7124±0.0053       33.93           9.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define candidate models with pipelines to avoid data leakage\n",
    "pipelines = {\n",
    "    'Naive Bayes': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2), \n",
    "                                    min_df=2, max_df=0.8, stop_words='english')),\n",
    "        ('clf', MultinomialNB(alpha=0.1))\n",
    "    ]),\n",
    "\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2), \n",
    "                                  min_df=2, max_df=0.8, stop_words='english')),\n",
    "        ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ]),\n",
    "    \n",
    "    'Random Forest': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2), \n",
    "                                  min_df=2, max_df=0.8, stop_words='english')),\n",
    "        ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Cross-validation step\n",
    "cv_results = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='f1')\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    cv_results[name] = (scores.mean(), scores.std(), duration)\n",
    "    print(f\"{name}: mean F1 = {scores.mean():.4f} ± {scores.std():.4f} \"\n",
    "          f\"(time: {duration:.2f} sec)\")\n",
    "\n",
    "# Train on full training set and evaluate on test set\n",
    "final_results = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    print(f\"\\nTraining {name} on full training set\")\n",
    "    start = time.time()\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1] if hasattr(pipe.named_steps['clf'], \"predict_proba\") else None\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    print(f\"{name} finished in {duration:.2f} sec\")\n",
    "    \n",
    "    final_results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        'cv_f1_mean': cv_results[name][0],\n",
    "        'cv_f1_std': cv_results[name][1],\n",
    "        'cv_time_sec': cv_results[name][2],\n",
    "        'train_time_sec': duration\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "metrics_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'Accuracy': f\"{res['accuracy']:.4f}\",\n",
    "        'Precision': f\"{res['precision']:.4f}\",\n",
    "        'Recall': f\"{res['recall']:.4f}\",\n",
    "        'F1-Score': f\"{res['f1']:.4f}\",\n",
    "        'ROC-AUC': f\"{res['roc_auc']:.4f}\" if res['roc_auc'] else \"N/A\",\n",
    "        'CV F1 (μ±σ)': f\"{res['cv_f1_mean']:.4f}±{res['cv_f1_std']:.4f}\",\n",
    "        'CV Time (s)': f\"{res['cv_time_sec']:.2f}\",\n",
    "        'Train Time (s)': f\"{res['train_time_sec']:.2f}\"\n",
    "    }\n",
    "    for name, res in final_results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\n=== Final Model Performance (with timings) ===\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ad006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f801c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece1762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bd2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df088f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43520b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
